{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74145dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check if PyTorch supports the built-in alternative (SDPA)\n",
    "print(hasattr(torch.nn.functional, 'scaled_dot_product_attention'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44e417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 49.8MB/s 0.1s.1s<0.0s\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a YOLOv8 model (you can replace 'yolov8n.pt' with your model path)\n",
    "model = YOLO('yolov8n.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50188fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = model.train(\n",
    "    data='data.yaml',  # Path to your config file\n",
    "    epochs=1,         # 50 runs through the data is a good start\n",
    "    imgsz=640,         # Resizes images to 640x640 pixels for training\n",
    "    batch=16,          # How many images to process at once (lower if you run out of memory)\n",
    "    device=0           # Use '0' for GPU, or 'cpu' if you don't have an NVIDIA GPU\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18526f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.234 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.233  Python-3.12.12 torch-2.9.1+cpu CPU (Intel Core Ultra 7 155H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.2 ms, read: 99.135.8 MB/s, size: 78.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\train\\labels.cache... 2599 images, 372 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2599/2599  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 84.746.7 MB/s, size: 65.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\labels.cache... 248 images, 18 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 248/248 247.8Kit/s 0.0s\n",
      "Plotting labels to C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/2         0G      2.161      2.972      1.551         13        640: 100% ━━━━━━━━━━━━ 163/163 8.9s/it 24:097.1ss8\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 6.6s/it 52.8s7.5ss\n",
      "                   all        248        446      0.356      0.334      0.252     0.0897\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/2         0G      2.126      2.432      1.575         13        640: 100% ━━━━━━━━━━━━ 163/163 8.2s/it 22:174.6ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 4.1s/it 32.8s4.7ss\n",
      "                   all        248        446      0.384      0.381      0.298     0.0979\n",
      "\n",
      "2 epochs completed in 0.798 hours.\n",
      "Optimizer stripped from C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.233  Python-3.12.12 torch-2.9.1+cpu CPU (Intel Core Ultra 7 155H)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 4.1s/it 32.5s4.2ss\n",
      "                   all        248        446      0.382      0.379      0.297     0.0982\n",
      "Speed: 1.3ms preprocess, 109.6ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\train2\u001b[0m\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = model.train(\n",
    "    data='data.yaml',  # Path to your config file\n",
    "    epochs=2,         # 50 runs through the data is a good start\n",
    "    imgsz=640,         # Resizes images to 640x640 pixels for training\n",
    "    batch=16,          # How many images to process at once (lower if you run out of memory)\n",
    "    # device=0           # Use '0' for GPU, or 'cpu' if you don't have an NVIDIA GPU\n",
    "    device='cpu'           # Use '0' for GPU, or 'cpu' if you don't have an NVIDIA GPU\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 363.4ms\n",
      "Speed: 10.7ms preprocess, 363.4ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Number of potholes detected: 0\n",
      "Saved output image!\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "# 1. Load your best trained model\n",
    "# model = YOLO('pothole_project/run_1/weights/best.pt')\n",
    "model = YOLO('yolo_project/train_v12_small/weights/best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fc3614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 256x640 (no detections), 130.3ms\n",
      "Speed: 18.7ms preprocess, 130.3ms inference, 0.7ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Number of potholes detected: 0\n",
      "Saved output image!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load Image\n",
    "# image = cv2.imread('test_road.jpg')\n",
    "# image = cv2.imread('trial/test126.jpg')\n",
    "# image = cv2.imread('trial/test146.jpg')\n",
    "image = cv2.imread('trial/testy001.jpg')\n",
    "\n",
    "# 3. Run YOLO Prediction\n",
    "results = model(image)[0]\n",
    "\n",
    "# 4. Convert to Supervision format (The Magic Step)\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "# --- REQUIREMENT: \"HOW MANY POTHOLES?\" ---\n",
    "pothole_count = len(detections)\n",
    "print(f\"Number of potholes detected: {pothole_count}\")\n",
    "\n",
    "# 5. Filter (Optional): Remove low confidence detections\n",
    "detections = detections[detections.confidence > 0.5]\n",
    "\n",
    "# 6. Draw Boxes and Labels\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "# Draw the boxes on the image\n",
    "annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "# Draw the labels (e.g., \"Pothole 0.85\")\n",
    "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "# 7. Show or Save\n",
    "cv2.imwrite('output_with_supervision.jpg', annotated_image)\n",
    "print(\"Saved output image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ce8ff",
   "metadata": {},
   "source": [
    "### Lower the confidence level and run against the test and validation datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bedb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model using the trained weights\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "# 1. Load your best trained model\n",
    "# model = YOLO('pothole_project/run_1/weights/best.pt')\n",
    "model = YOLO('yolo_project/train_v12_small/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a1ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.233  Python-3.12.12 torch-2.9.1+cpu CPU (Intel Core Ultra 7 155H)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv12s summary (fused): 159 layers, 9,231,267 parameters, 0 gradients, 21.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 3.11.1 MB/s, size: 69.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\labels.cache... 248 images, 18 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 248/248 123.6Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 16/16 6.0s/it 1:365.9ss\n",
      "                   all        248        446      0.754      0.552      0.644      0.367\n",
      "Speed: 1.1ms preprocess, 350.2ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\val\u001b[0m\n",
      "Mean Average Precision (mAP50): 0.6435270432605488\n"
     ]
    }
   ],
   "source": [
    "# Run Validation\n",
    "metrics = model.val(\n",
    "    data='data.yaml', \n",
    "    split='val',       # This tells it to use the 'val' folder defined in your yaml\n",
    "    conf=0.25,\n",
    ")\n",
    "\n",
    "print(f\"Mean Average Precision (mAP50): {metrics.box.map50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2378313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.233  Python-3.12.12 torch-2.9.1+cpu CPU (Intel Core Ultra 7 155H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 9.81.6 MB/s, size: 79.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\test\\labels... 128 images, 13 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 128/128 468.4it/s 0.3ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\test\\labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 7.3s/it 58.1s8.2ss\n",
      "                   all        128        203      0.652      0.522      0.599      0.372\n",
      "Speed: 1.6ms preprocess, 431.7ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\runs\\detect\\val2\u001b[0m\n",
      "Mean Average Precision (mAP50): 0.598603940887538\n"
     ]
    }
   ],
   "source": [
    "# Run Validation\n",
    "metrics = model.val(\n",
    "    data='data.yaml', \n",
    "    split='test',       # This tells it to use the 'test' folder defined in your yaml\n",
    "    conf=0.25,\n",
    ")\n",
    "\n",
    "print(f\"Mean Average Precision (mAP50): {metrics.box.map50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f145fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0106_jpg.rf.94ce9a4d57934a6767648b0de8f32153.jpg: 640x640 (no detections), 222.4ms\n",
      "image 2/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0122_jpg.rf.ce18c2ebe76966959cc6d314efa8e2a8.jpg: 640x640 13 0s, 153.5ms\n",
      "image 3/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0126_jpg.rf.e2a886338e6d02e063bcc11c8726ad45.jpg: 640x640 14 0s, 158.8ms\n",
      "image 4/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0149_jpg.rf.f89e5b8de933145d4d8a6654b5c84db6.jpg: 640x640 3 0s, 156.3ms\n",
      "image 5/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0151_jpg.rf.a067a83a8a33184931b64fdfc009eb49.jpg: 640x640 7 0s, 146.9ms\n",
      "image 6/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\0557_jpg.rf.915c05fc919568e7d069019d53faeeff.jpg: 640x640 49 0s, 147.5ms\n",
      "image 7/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\103_jpg.rf.d44230935194d30cb52d5ea9f77a6b06.jpg: 640x640 6 0s, 156.8ms\n",
      "image 8/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\117_jpg.rf.befd1ae740b5835e5ed741ea269a4a2c.jpg: 640x640 3 0s, 153.2ms\n",
      "image 9/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\120_jpg.rf.68a21d7a6a42f754e6a70d1eaa224628.jpg: 640x640 10 0s, 142.1ms\n",
      "image 10/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\124_jpg.rf.22d6f98fa8471af55c0f979249fad5cc.jpg: 640x640 27 0s, 145.7ms\n",
      "image 11/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\12_webp.rf.09a19b625570efa7c69d3d9cdf30e78c.jpg: 640x640 15 0s, 144.3ms\n",
      "image 12/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\137_jpg.rf.46e5047668507ad82fbe6ded515b643e.jpg: 640x640 12 0s, 143.1ms\n",
      "image 13/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\137_jpg.rf.db60cdda5004a582ef518483bc3c120e.jpg: 640x640 8 0s, 149.0ms\n",
      "image 14/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\145_jpg.rf.79dc872e82a947ad690ccc116d04832f.jpg: 640x640 7 0s, 148.9ms\n",
      "image 15/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\149_jpg.rf.898021ad5b2ce77486539084dabca175.jpg: 640x640 9 0s, 155.4ms\n",
      "image 16/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\170_jpg.rf.623e9e461379a992acddd617bc44fe42.jpg: 640x640 9 0s, 144.7ms\n",
      "image 17/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\242_jpg.rf.51146640d9b7ea1e0d07fb5488e2330b.jpg: 640x640 6 0s, 163.8ms\n",
      "image 18/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\471_png_jpg.rf.5c2bc951f8263845073297b6805a7e04.jpg: 640x640 16 0s, 175.3ms\n",
      "image 19/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\56_jpg.rf.0abe5299719d906d1ce5899c879adcab.jpg: 640x640 18 0s, 148.8ms\n",
      "image 20/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\699_png_jpg.rf.1fe145fdea228ac9b85139027d44f35a.jpg: 640x640 19 0s, 164.4ms\n",
      "image 21/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\82_jpg.rf.446bb1850098f3f584de9d6bdc273b7a.jpg: 640x640 2 0s, 157.5ms\n",
      "image 22/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\86_jpg.rf.b9ee0e108d56384cdb84609e8fd73017.jpg: 640x640 28 0s, 145.0ms\n",
      "image 23/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211008_093122_238_1_jpg.rf.811001d5479d88cd3c202568da13fe1b.jpg: 640x640 1 0, 153.6ms\n",
      "image 24/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211008_093122_453_3_jpg.rf.aa433a0ca5eb4581e75fdd35c0d9f416.jpg: 640x640 4 0s, 151.4ms\n",
      "image 25/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211008_093122_877_2_jpg.rf.b4d2f9c046bb3902c6584cee901e049a.jpg: 640x640 1 0, 151.3ms\n",
      "image 26/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211008_102249_451_2_jpg.rf.ae7803a2af507b654606b425aeb33809.jpg: 640x640 2 0s, 149.4ms\n",
      "image 27/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211008_102249_609_1_jpg.rf.74ce5dab0f3cb8777c7cce9666a4aecc.jpg: 640x640 1 0, 146.5ms\n",
      "image 28/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_429_2_jpg.rf.9a093856b053db6b3926e0c9237e7d9b.jpg: 640x640 1 0, 147.9ms\n",
      "image 29/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_445_2_jpg.rf.6ee4fd9a2763cd8c1575bd9c36947368.jpg: 640x640 1 0, 149.6ms\n",
      "image 30/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_450_3_jpg.rf.dee2a473eae9c987b4dde4ae20fd7342.jpg: 640x640 1 0, 151.6ms\n",
      "image 31/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_451_2_jpg.rf.1830aa9412f758ec8260f80ba0eace9c.jpg: 640x640 1 0, 150.3ms\n",
      "image 32/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_451_2_jpg.rf.693359f0f2acd5ec715e4a4045a2cd2d.jpg: 640x640 1 0, 151.3ms\n",
      "image 33/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_481_1_jpg.rf.ab43722e7e227f326ad8cd43c331b437.jpg: 640x640 1 0, 150.4ms\n",
      "image 34/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211012_094252_481_2_jpg.rf.974e278adc6c6c5a26901e03ca754a51.jpg: 640x640 2 0s, 142.5ms\n",
      "image 35/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_100234_34_3_jpg.rf.3a0e1ce4a2534169a84d15d9877b02be.jpg: 640x640 1 0, 162.2ms\n",
      "image 36/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_1041_2_jpg.rf.13bd15b7e437568724e29d591cc8d24d.jpg: 640x640 2 0s, 160.0ms\n",
      "image 37/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_1041_2_jpg.rf.94112427c95b7b233ef37df582c0e57a.jpg: 640x640 2 0s, 149.0ms\n",
      "image 38/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_136_3_jpg.rf.4fb28da327f0787d92831e8a15e2db34.jpg: 640x640 2 0s, 161.9ms\n",
      "image 39/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_147_1_jpg.rf.1798a1cc7c4331c6282be84d4cde67c7.jpg: 640x640 2 0s, 161.4ms\n",
      "image 40/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_162_3_jpg.rf.73f87f73d4a4a55e4cd01baf104448ed.jpg: 640x640 3 0s, 156.0ms\n",
      "image 41/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_162_3_jpg.rf.e8742a6f13ab297be63185a7c502c249.jpg: 640x640 4 0s, 146.1ms\n",
      "image 42/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_164_2_jpg.rf.30c97e905342794aeacaed5f864b7fa9.jpg: 640x640 2 0s, 170.3ms\n",
      "image 43/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_164_3_jpg.rf.41d9baaea4648255dce0eec23a6c2c9f.jpg: 640x640 2 0s, 147.3ms\n",
      "image 44/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_214_3_jpg.rf.22ea090fdfce6182609aa0f389451318.jpg: 640x640 3 0s, 161.5ms\n",
      "image 45/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_606_1_jpg.rf.453b4a9400bb7095dc49e9116d8cbc37.jpg: 640x640 4 0s, 152.1ms\n",
      "image 46/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_606_1_jpg.rf.e3e5ffcb6c2d9e7e41d422453a181e49.jpg: 640x640 2 0s, 160.2ms\n",
      "image 47/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_772_1_jpg.rf.3f5c3b048561c2f9a35d8256e7e84c3f.jpg: 640x640 (no detections), 161.3ms\n",
      "image 48/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_772_1_jpg.rf.51ea5df3e795c5f8c05da46a86e39e85.jpg: 640x640 1 0, 164.0ms\n",
      "image 49/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_773_2_jpg.rf.235e593ef9eaa645aa2b3623682d3d41.jpg: 640x640 1 0, 157.1ms\n",
      "image 50/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_917_1_jpg.rf.07c8b47f603a9eb0b01507ea46addaf6.jpg: 640x640 1 0, 161.1ms\n",
      "image 51/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_917_1_jpg.rf.35a07ae08fbcc5f8e29d577e83d99813.jpg: 640x640 2 0s, 161.8ms\n",
      "image 52/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_947_2_jpg.rf.e7bcf0b8e9979055b7bc1e13ce7429f9.jpg: 640x640 1 0, 146.8ms\n",
      "image 53/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211013_101138_948_3_jpg.rf.25cc8051931e33f4e3fadce7d8f4e321.jpg: 640x640 1 0, 156.8ms\n",
      "image 54/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_093601_1555_3_jpg.rf.8f1145130d112f621f7e3ab50ca17b5c.jpg: 640x640 3 0s, 157.1ms\n",
      "image 55/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_093601_1556_1_jpg.rf.d0dc2fbaa71e9f851f5bda8e7fb07caf.jpg: 640x640 2 0s, 158.6ms\n",
      "image 56/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_093601_1556_3_jpg.rf.a5b71815917db3f5a243b27f35c90292.jpg: 640x640 1 0, 152.8ms\n",
      "image 57/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_093601_185_2_jpg.rf.9381efddeb91805aef1fcadaa5189c49.jpg: 640x640 1 0, 153.1ms\n",
      "image 58/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_104914_1133_2_jpg.rf.19c338e69cf53d6beba98e811c909ddb.jpg: 640x640 1 0, 149.1ms\n",
      "image 59/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_sunny_CI04_20211014_104914_1133_2_jpg.rf.f62be6ee7cf25f8962c6f8298300887a.jpg: 640x640 2 0s, 154.3ms\n",
      "image 60/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_wet_CI04_20210930_092647_597_1_jpg.rf.ce510d0d82eb9d0fb876ed58b07d9449.jpg: 640x640 6 0s, 150.4ms\n",
      "image 61/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_wet_CI04_20210930_102127_1437_2_jpg.rf.9453e3a78db933c2b4aa334af0734979.jpg: 640x640 5 0s, 155.6ms\n",
      "image 62/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\AM_wet_CI04_20211021_111419_90_3_jpg.rf.438eba9b2e70a07b678bce87fd61ae6a.jpg: 640x640 1 0, 159.3ms\n",
      "image 63/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0014_jpg.rf.e46ae99780cd021c05375d182b074e86.jpg: 640x640 4 0s, 173.9ms\n",
      "image 64/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0015_jpg.rf.7172d035c62215fec6b06117294177f6.jpg: 640x640 5 0s, 167.9ms\n",
      "image 65/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0016_jpg.rf.06b817e937d23b7d4365d6fb19e62a19.jpg: 640x640 3 0s, 170.8ms\n",
      "image 66/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0023_jpg.rf.cc1179c5d5960b290539054903724b48.jpg: 640x640 6 0s, 182.9ms\n",
      "image 67/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0025_jpg.rf.c57aea97b639a6df585ba9aff7d483dc.jpg: 640x640 6 0s, 175.9ms\n",
      "image 68/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0036_jpg.rf.9ec96f51eb49d364bd64640a21f5ac22.jpg: 640x640 1 0, 152.0ms\n",
      "image 69/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0045_jpg.rf.b04bc942fac2d42e18bc234ec8af39a4.jpg: 640x640 3 0s, 166.1ms\n",
      "image 70/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0058_jpg.rf.10be51c908d23c28aaa429431b837d12.jpg: 640x640 13 0s, 200.3ms\n",
      "image 71/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0064_jpg.rf.79f22d33ed566fa71d441e48b1fe4e02.jpg: 640x640 12 0s, 178.5ms\n",
      "image 72/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\British-Road-Overrun-With-Potholes_mp4-0067_jpg.rf.c55c10bc7532cd257e62e060733f54b0.jpg: 640x640 12 0s, 165.4ms\n",
      "image 73/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0008_jpg.rf.c44611ec4ec23d9d0511859ebf833d84.jpg: 640x640 15 0s, 165.8ms\n",
      "image 74/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0009_jpg.rf.4c33ad737794bc74e1ed2e2d79b49aa8.jpg: 640x640 6 0s, 167.8ms\n",
      "image 75/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0014_jpg.rf.d68ee1674bfe1a74b6cdc1c336669c60.jpg: 640x640 11 0s, 179.7ms\n",
      "image 76/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0016_jpg.rf.7d2a6aaf6e35491e58b51add72856e09.jpg: 640x640 14 0s, 194.8ms\n",
      "image 77/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0040_jpg.rf.8f3aec14ba37189996fab4484fa361e6.jpg: 640x640 1 0, 158.6ms\n",
      "image 78/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0061_jpg.rf.1df76aa8c0a9ace59cf2a94d9bbf4052.jpg: 640x640 9 0s, 157.6ms\n",
      "image 79/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0073_jpg.rf.789d877c2581a6abdee7ef726c7ea44c.jpg: 640x640 7 0s, 178.8ms\n",
      "image 80/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0083_jpg.rf.a65b62b1be38bca1b924455b146ffb0b.jpg: 640x640 4 0s, 186.4ms\n",
      "image 81/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0087_jpg.rf.984351a4a6e2ad29aca86e1df4475959.jpg: 640x640 4 0s, 193.2ms\n",
      "image 82/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0118_jpg.rf.3617254bf2f0526ca443894633a9b08d.jpg: 640x640 (no detections), 186.3ms\n",
      "image 83/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0155_jpg.rf.d7f8191241b8dee63f13ca0dbd0f984a.jpg: 640x640 6 0s, 187.6ms\n",
      "image 84/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0190_jpg.rf.29f4edfc12a8238c9a20826044289ea9.jpg: 640x640 2 0s, 205.3ms\n",
      "image 85/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0205_jpg.rf.dc053ad588b0a44f378e23943b9bb51d.jpg: 640x640 7 0s, 189.5ms\n",
      "image 86/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0222_jpg.rf.902db90d791ac88bd6044db61b4b9a4b.jpg: 640x640 3 0s, 201.6ms\n",
      "image 87/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0223_jpg.rf.0dfd413d754967936f34623977e272a9.jpg: 640x640 8 0s, 210.9ms\n",
      "image 88/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0231_jpg.rf.4dc850d6c7a7eeb095c20df37bb110ae.jpg: 640x640 2 0s, 219.1ms\n",
      "image 89/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0232_jpg.rf.aa977d4f401c6bbd76b98cd0edf67aa1.jpg: 640x640 4 0s, 229.0ms\n",
      "image 90/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0238_jpg.rf.4eade3459a4cfd79391b22a61156d164.jpg: 640x640 8 0s, 219.9ms\n",
      "image 91/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0241_jpg.rf.25d3443d74cd7bcc109efe3a79312f66.jpg: 640x640 9 0s, 200.4ms\n",
      "image 92/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0251_jpg.rf.c571a3b53db8fd3e9f34a4b8eac53353.jpg: 640x640 7 0s, 191.1ms\n",
      "image 93/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0271_jpg.rf.626307202e71ad216338836cff0cf5f6.jpg: 640x640 9 0s, 194.1ms\n",
      "image 94/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0279_jpg.rf.d8bda3195f8b7c1b886c557d14639b11.jpg: 640x640 18 0s, 205.1ms\n",
      "image 95/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0280_jpg.rf.c2ad67a6d83f06b38225dff4a697db79.jpg: 640x640 4 0s, 200.4ms\n",
      "image 96/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0285_jpg.rf.f3f265b8cbd2cbcb12d9042837eda516.jpg: 640x640 15 0s, 204.0ms\n",
      "image 97/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0287_jpg.rf.696781f2863d37c306c58f7788b66d5f.jpg: 640x640 3 0s, 209.9ms\n",
      "image 98/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0289_jpg.rf.fa4626b8c953027052d0709e3bd3a91b.jpg: 640x640 10 0s, 216.4ms\n",
      "image 99/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0294_jpg.rf.3104469af79cfe142dff22b82de33afa.jpg: 640x640 4 0s, 217.2ms\n",
      "image 100/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0314_jpg.rf.664ec8bf9a17a9e5bd9b4e1ad4d88122.jpg: 640x640 3 0s, 204.3ms\n",
      "image 101/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0318_jpg.rf.88bde2937256f7870672eb610225d4f4.jpg: 640x640 6 0s, 200.0ms\n",
      "image 102/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0422_jpg.rf.a096a75d86219d8904c89da19c9c8bd7.jpg: 640x640 4 0s, 207.1ms\n",
      "image 103/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0457_jpg.rf.67c405243353d04bf9440be66d586074.jpg: 640x640 9 0s, 190.0ms\n",
      "image 104/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0460_jpg.rf.d1a628d7deb9ec0d9b3c90f25463d620.jpg: 640x640 6 0s, 226.8ms\n",
      "image 105/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0463_jpg.rf.76e41b4abd796225d8c14123ed408543.jpg: 640x640 12 0s, 206.5ms\n",
      "image 106/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0465_jpg.rf.289d4f6b97b981fed6bc564c9adc8fa2.jpg: 640x640 5 0s, 204.0ms\n",
      "image 107/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0471_jpg.rf.87ad82d619d4adda5976d5f7ec17e8f4.jpg: 640x640 5 0s, 202.1ms\n",
      "image 108/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0477_jpg.rf.ec7dfd263473db566d43c377c7d12c17.jpg: 640x640 9 0s, 205.0ms\n",
      "image 109/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0481_jpg.rf.0d303dc4fe33f0e3143dc450bc6947ed.jpg: 640x640 2 0s, 222.1ms\n",
      "image 110/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0482_jpg.rf.f4afbf9839f59adc2495c821f16cb16d.jpg: 640x640 1 0, 207.1ms\n",
      "image 111/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0483_jpg.rf.470fd83314a92e4eab88792c70a29793.jpg: 640x640 4 0s, 211.3ms\n",
      "image 112/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0486_jpg.rf.3f2a282aae73ca57f9087a56a4026a0e.jpg: 640x640 6 0s, 212.4ms\n",
      "image 113/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0488_jpg.rf.f271cad238ccfdafa746301037abe2c0.jpg: 640x640 9 0s, 236.5ms\n",
      "image 114/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0495_jpg.rf.392c9e6f83cddfe34bf67da31c5889ea.jpg: 640x640 4 0s, 224.6ms\n",
      "image 115/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0496_jpg.rf.f35102be694b39cd21fb38ec4f7e7272.jpg: 640x640 1 0, 214.7ms\n",
      "image 116/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0501_jpg.rf.16c783bc279216d1a513d41647d9ae85.jpg: 640x640 1 0, 215.6ms\n",
      "image 117/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0502_jpg.rf.fede10120a80e717103ec62076cc5070.jpg: 640x640 4 0s, 212.1ms\n",
      "image 118/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0503_jpg.rf.c5ed862bb3df0516c72ff166560fbb18.jpg: 640x640 3 0s, 211.5ms\n",
      "image 119/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0504_jpg.rf.2f83162799dbb406fbbd92c419fe591c.jpg: 640x640 7 0s, 224.4ms\n",
      "image 120/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0505_jpg.rf.cd6cfa95b405eb1f3eb6a339348a81f1.jpg: 640x640 9 0s, 215.4ms\n",
      "image 121/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0506_jpg.rf.90e9b5ac525bc50461dfa6a6efee46a7.jpg: 640x640 2 0s, 215.8ms\n",
      "image 122/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0509_jpg.rf.8e3dc1f748370184c16733da50db6e29.jpg: 640x640 13 0s, 210.2ms\n",
      "image 123/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0517_jpg.rf.bd026473fd73737eb4974863fe6f0e3d.jpg: 640x640 12 0s, 221.7ms\n",
      "image 124/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0524_jpg.rf.eb6426ca34096d4f90b6fee99a6532c3.jpg: 640x640 8 0s, 208.6ms\n",
      "image 125/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0526_jpg.rf.ba7dc72ba040c6dba6042c3ca3c1c969.jpg: 640x640 6 0s, 232.1ms\n",
      "image 126/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0538_jpg.rf.35a482c177b829085e460dfb6c623e4e.jpg: 640x640 1 0, 216.8ms\n",
      "image 127/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0548_jpg.rf.5fcd916f98dd4f54aa2298e9409b2b7d.jpg: 640x640 5 0s, 224.3ms\n",
      "image 128/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0589_jpg.rf.0b012ae0989202813623d69c1e18abc2.jpg: 640x640 5 0s, 221.2ms\n",
      "image 129/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0618_jpg.rf.778b425d152353403e8b641543f9fe42.jpg: 640x640 4 0s, 219.5ms\n",
      "image 130/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0620_jpg.rf.c61d6bbd25fb5490c0fb6ceaef4217b1.jpg: 640x640 1 0, 249.2ms\n",
      "image 131/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0621_jpg.rf.85a24ba569b073f40a9b8a2f76d9f561.jpg: 640x640 13 0s, 233.1ms\n",
      "image 132/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0623_jpg.rf.8976e3d24a226bb3e07649fa42a0b0f1.jpg: 640x640 1 0, 225.6ms\n",
      "image 133/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0648_jpg.rf.69f62cf17f18fdd8fcdb48c48fac0b7c.jpg: 640x640 13 0s, 217.9ms\n",
      "image 134/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH-CAM-2016-01-29-42-Miles-of-Potholes-_mp4-0650_jpg.rf.0c9100c10125c17badf3c1164b368eab.jpg: 640x640 3 0s, 222.6ms\n",
      "image 135/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-10_jpg.rf.c461a22486e87f85c8f2fabe1d25ed4e.jpg: 640x640 9 0s, 220.3ms\n",
      "image 136/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-133_jpg.rf.d2890795faa4fb4c42cf231ac2ef9624.jpg: 640x640 9 0s, 221.8ms\n",
      "image 137/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-139_jpg.rf.fdc56eb5089a8ad830eca33d01b6cf6b.jpg: 640x640 6 0s, 212.0ms\n",
      "image 138/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-187_jpg.rf.977ffc33e09846605b8c1b5e8fce5b77.jpg: 640x640 4 0s, 221.0ms\n",
      "image 139/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-193_jpg.rf.468416d8eee54212c41d070df05d382f.jpg: 640x640 2 0s, 215.1ms\n",
      "image 140/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-204_jpg.rf.e33c77a3764fad4a491109e133c51e6b.jpg: 640x640 5 0s, 225.6ms\n",
      "image 141/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-231_jpg.rf.209e56d336f475c7dce9c4f07f681ba2.jpg: 640x640 1 0, 229.8ms\n",
      "image 142/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-246_jpg.rf.c6b5e7ee5077d6058a23dcf992bd2bb2.jpg: 640x640 6 0s, 224.3ms\n",
      "image 143/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-258_jpg.rf.9caff96f5443abab7559309b50b6976b.jpg: 640x640 2 0s, 214.7ms\n",
      "image 144/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-276_jpg.rf.7e421abb91c573b3dc6aaec383f4c887.jpg: 640x640 4 0s, 234.0ms\n",
      "image 145/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-277_jpg.rf.bbc15a9bed32d8424073367b9c2f6238.jpg: 640x640 7 0s, 236.7ms\n",
      "image 146/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-277_jpg.rf.c290c429a3572286a60c5f9ab22ad28c.jpg: 640x640 5 0s, 231.9ms\n",
      "image 147/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-279_jpg.rf.1fd954e6baacc47b8dd595005d69f616.jpg: 640x640 9 0s, 235.2ms\n",
      "image 148/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-286_jpg.rf.80d019885ca601b0b2b2a94a0f23d4c6.jpg: 640x640 4 0s, 223.4ms\n",
      "image 149/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-289_jpg.rf.68da38e9dc0191e61c583d87bdd689be.jpg: 640x640 3 0s, 227.4ms\n",
      "image 150/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-293_jpg.rf.f91c11f6fd9cedb28babed0a155b644a.jpg: 640x640 2 0s, 247.1ms\n",
      "image 151/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-298_jpg.rf.eb0138a24038ed18d81df9b803c1820b.jpg: 640x640 5 0s, 222.7ms\n",
      "image 152/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-299_jpg.rf.726765cef3cf225af5d5be7a944c5f4c.jpg: 640x640 3 0s, 240.3ms\n",
      "image 153/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-308_jpg.rf.15d761b7bf04550893ffb5e565ff479e.jpg: 640x640 1 0, 248.8ms\n",
      "image 154/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-311_jpg.rf.0c5cd5bad06b34a2a9cb0e6479d9cb71.jpg: 640x640 1 0, 263.2ms\n",
      "image 155/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-311_jpg.rf.0d51870390eb3d1723efcda403c59f19.jpg: 640x640 2 0s, 224.9ms\n",
      "image 156/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-319_jpg.rf.f16a62722bdb8ba077c2dcd070934db1.jpg: 640x640 6 0s, 233.8ms\n",
      "image 157/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-325_jpg.rf.8d9cc1b409f5b750f353ee010e6d8032.jpg: 640x640 7 0s, 236.2ms\n",
      "image 158/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-340_jpg.rf.78afb75e56bdb63eff6ef3be35150f8d.jpg: 640x640 1 0, 256.2ms\n",
      "image 159/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-341_jpg.rf.2737939c10cf160a87d5b0027fff9168.jpg: 640x640 1 0, 249.1ms\n",
      "image 160/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-349_jpg.rf.a54f752ea3ba1a1f34b592d241271275.jpg: 640x640 1 0, 247.2ms\n",
      "image 161/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-377_jpg.rf.44f71448b1b251cdebc12194cabfdf50.jpg: 640x640 3 0s, 259.3ms\n",
      "image 162/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-382_jpg.rf.4f48afc19e2259de29bf3fb96cae182a.jpg: 640x640 3 0s, 267.8ms\n",
      "image 163/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-40_jpg.rf.71443ddab912bac40a8c52623370e8a7.jpg: 640x640 1 0, 252.0ms\n",
      "image 164/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-417_jpg.rf.0f4eb2180ab167e8fb4ce2fc96b47716.jpg: 640x640 8 0s, 264.4ms\n",
      "image 165/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-425_jpg.rf.c9af918bf860ff0b4a7d17173ec2ac0f.jpg: 640x640 3 0s, 282.4ms\n",
      "image 166/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-426_jpg.rf.96aa0de711db382fe14a9555e49d080f.jpg: 640x640 5 0s, 285.8ms\n",
      "image 167/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-463_jpg.rf.ba66a3e599ffaa91df0c92356e22fe84.jpg: 640x640 14 0s, 275.5ms\n",
      "image 168/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-465_jpg.rf.981f9eb555ed474d5a0123f910e0a483.jpg: 640x640 3 0s, 262.7ms\n",
      "image 169/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-487_jpg.rf.2016026f24e73b00498e83c3b80f5d3f.jpg: 640x640 6 0s, 264.5ms\n",
      "image 170/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-491_jpg.rf.0a621f0b0f2ff544a17a6a15bded4fb8.jpg: 640x640 4 0s, 257.4ms\n",
      "image 171/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-503_jpg.rf.70e3554d2b23500c0761cd5edc0d1ad2.jpg: 640x640 4 0s, 249.8ms\n",
      "image 172/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-504_jpg.rf.7c407b9eb4bd035ea154275292f62b6b.jpg: 640x640 3 0s, 238.7ms\n",
      "image 173/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-507_jpg.rf.05d314168b5cbe957e5d91cae8ea36a7.jpg: 640x640 3 0s, 244.3ms\n",
      "image 174/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-512_jpg.rf.4ddbcac745de1008696eef47325893df.jpg: 640x640 17 0s, 240.2ms\n",
      "image 175/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-513_jpg.rf.71d3948fcbd855beb5d10ed31d667b88.jpg: 640x640 5 0s, 219.5ms\n",
      "image 176/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-514_jpg.rf.6d67b1619f646a9f58e39b3269d3d867.jpg: 640x640 1 0, 230.3ms\n",
      "image 177/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-516_jpg.rf.84442358d3f2641a1ab25096c101d384.jpg: 640x640 3 0s, 229.9ms\n",
      "image 178/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-518_jpg.rf.13363faded8b9ab5ae087ac09555aa1d.jpg: 640x640 6 0s, 233.8ms\n",
      "image 179/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-521_jpg.rf.9f570cb2c76540b5ad164494189c2464.jpg: 640x640 7 0s, 251.5ms\n",
      "image 180/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-525_jpg.rf.06edcc5727d7f63e1f6394e8e43d6792.jpg: 640x640 11 0s, 227.6ms\n",
      "image 181/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-529_jpg.rf.1a294542021f765ec4d397dfc746f5f3.jpg: 640x640 5 0s, 234.1ms\n",
      "image 182/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-555_jpg.rf.3f2e1e60f9b1526658de4c17e2cbf2c9.jpg: 640x640 1 0, 246.6ms\n",
      "image 183/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-561_jpg.rf.8258fbab9b23c3eb8c0b9b41eef038d6.jpg: 640x640 6 0s, 246.3ms\n",
      "image 184/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-584_jpg.rf.22e8a45798173d5a9cb3477ed70251d7.jpg: 640x640 3 0s, 241.0ms\n",
      "image 185/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-585_jpg.rf.b705fd108eb607c9579c1bf4a7f9467c.jpg: 640x640 2 0s, 247.6ms\n",
      "image 186/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-589_jpg.rf.89e9d43580a1687e873eae0626a5be3e.jpg: 640x640 5 0s, 281.3ms\n",
      "image 187/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-600_jpg.rf.f24c4f1491f0da4e305d4f5ec3f411b2.jpg: 640x640 3 0s, 261.1ms\n",
      "image 188/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-612_jpg.rf.a0016936ae612d3ebb2cff224808964f.jpg: 640x640 1 0, 251.1ms\n",
      "image 189/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-62_jpg.rf.254697a4cbdab3e2cf10b494859ee9ff.jpg: 640x640 2 0s, 276.6ms\n",
      "image 190/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-62_jpg.rf.25dc9cbf8a8dd5e8c145f46eceaee737.jpg: 640x640 2 0s, 252.2ms\n",
      "image 191/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-638_jpg.rf.ee011ed501315df279ac04bde651fc3d.jpg: 640x640 1 0, 272.9ms\n",
      "image 192/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-649_jpg.rf.0a57d25362ccd727cd7d732c41c45a0f.jpg: 640x640 6 0s, 293.7ms\n",
      "image 193/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-6_jpg.rf.43763e5223fd470847e95dafdd4556ac.jpg: 640x640 4 0s, 290.0ms\n",
      "image 194/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-70_jpg.rf.cb8f6b47d9e4ff8612a509d0764ecdeb.jpg: 640x640 3 0s, 299.3ms\n",
      "image 195/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-74_jpg.rf.6c857297872e4b8c3278e03c439cb75a.jpg: 640x640 1 0, 264.3ms\n",
      "image 196/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-83_jpg.rf.1f872e5c235e977bcf2f8cc233572b92.jpg: 640x640 1 0, 266.7ms\n",
      "image 197/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-83_jpg.rf.ed705f7da821279c640772b5bcb10a72.jpg: 640x640 2 0s, 284.3ms\n",
      "image 198/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-87_jpg.rf.06bc1e91e636f0230aa67f1319ca9513.jpg: 640x640 1 0, 294.6ms\n",
      "image 199/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-87_jpg.rf.2d6b219d7265664b723ef46fc08d61ed.jpg: 640x640 1 0, 297.1ms\n",
      "image 200/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\DASH_CAM_2016_01_29_-42_Miles_of_Potholes-_mp4-95_jpg.rf.25f43f5b8c7346169ccd83726824fcdd.jpg: 640x640 2 0s, 259.0ms\n",
      "image 201/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0001_jpg.rf.d3e36757487b45372d37f40fdadd3542.jpg: 640x640 4 0s, 340.3ms\n",
      "image 202/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0002_jpg.rf.a18af4f81129e79e5e11441874c3a71a.jpg: 640x640 3 0s, 343.3ms\n",
      "image 203/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0005_jpg.rf.74d54c2dd0374886edadcbd856823f03.jpg: 640x640 5 0s, 313.1ms\n",
      "image 204/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0010_jpg.rf.c6e5766cc8733159b5d4aa875aad465f.jpg: 640x640 (no detections), 358.6ms\n",
      "image 205/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0011_jpg.rf.247b7254f0dae6ca0f0a8eb480c199dd.jpg: 640x640 5 0s, 323.7ms\n",
      "image 206/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0015_jpg.rf.e1f5aa1b65295b5c4934f25ae64b597c.jpg: 640x640 6 0s, 377.3ms\n",
      "image 207/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0020_jpg.rf.764f7457865b5d3f335ad9c402915961.jpg: 640x640 7 0s, 335.8ms\n",
      "image 208/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0026_jpg.rf.594d2710370e6ac8ed6960a2830ddad6.jpg: 640x640 4 0s, 352.3ms\n",
      "image 209/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0050_jpg.rf.1417fc4b963a4a0e03fa8fd1fb77c587.jpg: 640x640 (no detections), 387.7ms\n",
      "image 210/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0108_jpg.rf.2f9f34994faeb11c864294543459d8a4.jpg: 640x640 5 0s, 376.8ms\n",
      "image 211/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0117_jpg.rf.0fc5d5388abf6a7504d5f73c9c33dce4.jpg: 640x640 11 0s, 368.6ms\n",
      "image 212/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dangerously-Dodging-Potholes-on-17th-Top-City-Dash-Cam-06_29_2018_mp4-0156_jpg.rf.e509275664ef3224fbbefc885d3405fc.jpg: 640x640 10 0s, 386.5ms\n",
      "image 213/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0008_jpg.rf.58145670b2a5691c7cb1c7d394e2f04c.jpg: 640x640 1 0, 365.9ms\n",
      "image 214/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0018_jpg.rf.d98d6e162846be5f617630101ec49079.jpg: 640x640 2 0s, 391.1ms\n",
      "image 215/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0024_jpg.rf.b6f834553ca81042a8d9c87d4f19a7f7.jpg: 640x640 3 0s, 403.1ms\n",
      "image 216/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0028_jpg.rf.83d35930d86c6fd34505ccc805434e55.jpg: 640x640 3 0s, 428.9ms\n",
      "image 217/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0035_jpg.rf.5104af31ace0d0ecdb402f622be5c445.jpg: 640x640 3 0s, 548.5ms\n",
      "image 218/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0036_jpg.rf.a8c0c385c06005efe9860498d6fbb775.jpg: 640x640 (no detections), 444.3ms\n",
      "image 219/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0039_jpg.rf.5776dbef97c341e809be0e40a8a0646e.jpg: 640x640 2 0s, 439.1ms\n",
      "image 220/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Dash-Cam-Moments-Caught-on-Camera-_-Road-Eye-Footage-_-Part-277_mp4-0051_jpg.rf.b8bd9ad0ff1f16794d254b083bee9077.jpg: 640x640 (no detections), 390.3ms\n",
      "image 221/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Hit-a-pothole-at-70mph-Tire-Damaged_mp4-0027_jpg.rf.780abc169ed05ea95f1f999ced3767db.jpg: 640x640 (no detections), 357.1ms\n",
      "image 222/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Hit-a-pothole-at-70mph-Tire-Damaged_mp4-0038_jpg.rf.2e55845ffb9e549872efdaa9b0aea0ee.jpg: 640x640 1 0, 338.4ms\n",
      "image 223/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Hit-a-pothole-at-70mph-Tire-Damaged_mp4-0043_jpg.rf.294a8aed7be9f078c179eabf6e40581c.jpg: 640x640 1 0, 294.4ms\n",
      "image 224/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Hit-a-pothole-at-70mph-Tire-Damaged_mp4-0051_jpg.rf.3fb7c0545d843f7af5d808c95af66896.jpg: 640x640 (no detections), 311.3ms\n",
      "image 225/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Pothole-test-of-dash-cam-sensor_mp4-0051_jpg.rf.007c8ca8a071923faf2d967f8804f713.jpg: 640x640 1 0, 330.9ms\n",
      "image 226/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Pothole-test-of-dash-cam-sensor_mp4-0060_jpg.rf.837b1f15f4c011e5a2b05f329f09e492.jpg: 640x640 6 0s, 332.8ms\n",
      "image 227/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Pothole-test-of-dash-cam-sensor_mp4-0071_jpg.rf.2b93f2ebfd69ecc134608c369e104f2c.jpg: 640x640 5 0s, 353.5ms\n",
      "image 228/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Pothole-test-of-dash-cam-sensor_mp4-0074_jpg.rf.d6399be6727e7770c9ece9808bd69348.jpg: 640x640 1 0, 349.3ms\n",
      "image 229/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Tony-s-DashCam-How-many-potholes-can-you-spot-on-this-road_-59_mp4-0022_jpg.rf.6de59b558db57a4e6e1f61e6e600fd2f.jpg: 640x640 6 0s, 373.6ms\n",
      "image 230/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Yi-Smart-Dash-Cam-video-The-biggest-pothole-in-the-world_mp4-0017_jpg.rf.f6c881281bb79ad1069b8f358c7bf04b.jpg: 640x640 6 0s, 354.3ms\n",
      "image 231/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Yi-Smart-Dash-Cam-video-The-biggest-pothole-in-the-world_mp4-0035_jpg.rf.4a6354cacc46be2c8ddd29cd6bd06b57.jpg: 640x640 16 0s, 412.8ms\n",
      "image 232/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Yi-Smart-Dash-Cam-video-The-biggest-pothole-in-the-world_mp4-0037_jpg.rf.55e416971417fea429e12230cb3606b1.jpg: 640x640 17 0s, 448.6ms\n",
      "image 233/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Yi-Smart-Dash-Cam-video-The-biggest-pothole-in-the-world_mp4-0045_jpg.rf.cd1fc4d41ae534b3147b2142d3a59b95.jpg: 640x640 2 0s, 402.2ms\n",
      "image 234/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\Yi-Smart-Dash-Cam-video-The-biggest-pothole-in-the-world_mp4-0049_jpg.rf.36a9f88d5373bfe89af023773385eb42.jpg: 640x640 1 0, 428.4ms\n",
      "image 235/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\roadSample-1-_mp4-0004_jpg.rf.262165beaa310f06bae71210b4964778.jpg: 640x640 (no detections), 423.5ms\n",
      "image 236/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\roadSample-1-_mp4-0007_jpg.rf.5a2d42bc9c4ab779b401c5e72dd2574b.jpg: 640x640 8 0s, 383.6ms\n",
      "image 237/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\roadSample-1-_mp4-0027_jpg.rf.8f71c84c4958622ad91efa0de129431c.jpg: 640x640 (no detections), 370.3ms\n",
      "image 238/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\test2_png.rf.a5b5fa1ad06e98769456422825b2e581.jpg: 640x640 4 0s, 344.1ms\n",
      "image 239/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0011_jpg.rf.5fe3dff3594568fd7d27339031147d6c.jpg: 640x640 4 0s, 396.4ms\n",
      "image 240/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0013_jpg.rf.12abf5814bb2b092bcc4ae7441e7b23c.jpg: 640x640 3 0s, 352.6ms\n",
      "image 241/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0013_jpg.rf.31e8ca2b02f136ee14beec0500c31244.jpg: 640x640 (no detections), 359.5ms\n",
      "image 242/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0014_jpg.rf.2d5847104bbfbfe4ea29c60b9460daa1.jpg: 640x640 (no detections), 398.5ms\n",
      "image 243/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0015_jpg.rf.7a42d96f057c909f864343c3765ba9f9.jpg: 640x640 1 0, 394.9ms\n",
      "image 244/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0022_jpg.rf.d54c00ac5fdc0a2d473d57add5a86ddc.jpg: 640x640 (no detections), 373.2ms\n",
      "image 245/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0025_jpg.rf.b1e3c984945a69a00eebf460027315fd.jpg: 640x640 1 0, 369.2ms\n",
      "image 246/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0036_jpg.rf.44f6178a044c49affbe61555675c91d8.jpg: 640x640 (no detections), 301.3ms\n",
      "image 247/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0038_jpg.rf.da528670082e665a440aac18290e28d7.jpg: 640x640 2 0s, 296.8ms\n",
      "image 248/248 c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\data\\valid\\images\\videoplayback-1-_mp4-0047_jpg.rf.7f8424e1dc57143426cc1c37e1fd7067.jpg: 640x640 (no detections), 318.3ms\n",
      "Speed: 5.4ms preprocess, 235.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to c:\\DataA\\Centennial\\2025_Fall\\Neural_COMP258\\assignments\\group_project\\yolo\\prediction_results_val.csv\n",
      "Prediction processing complete!\n",
      "                                         image_name  original_count  \\\n",
      "0  0106_jpg.rf.94ce9a4d57934a6767648b0de8f32153.jpg               0   \n",
      "1  0122_jpg.rf.ce18c2ebe76966959cc6d314efa8e2a8.jpg              13   \n",
      "2  0126_jpg.rf.e2a886338e6d02e063bcc11c8726ad45.jpg              14   \n",
      "3  0149_jpg.rf.f89e5b8de933145d4d8a6654b5c84db6.jpg               3   \n",
      "4  0151_jpg.rf.a067a83a8a33184931b64fdfc009eb49.jpg               7   \n",
      "\n",
      "   filtered_count                                     original_boxes  \\\n",
      "0               0                                                 []   \n",
      "1               3  [[134.90692, 549.4906, 258.1073, 584.6964], [0...   \n",
      "2               2  [[503.093, 392.3219, 611.3562, 426.67798], [23...   \n",
      "3               1  [[594.5593, 624.6094, 639.13477, 640.0], [338....   \n",
      "4               3  [[224.2022, 335.6172, 338.6812, 417.51843], [2...   \n",
      "\n",
      "                                      filtered_boxes  \\\n",
      "0                                                 []   \n",
      "1  [[134.90692, 549.4906, 258.1073, 584.6964], [0...   \n",
      "2  [[503.093, 392.3219, 611.3562, 426.67798], [23...   \n",
      "3           [[594.5593, 624.6094, 639.13477, 640.0]]   \n",
      "4  [[224.2022, 335.6172, 338.6812, 417.51843], [2...   \n",
      "\n",
      "                            confidences  gt_count  \\\n",
      "0                                    []         2   \n",
      "1    [0.4797452, 0.2816665, 0.14435862]         4   \n",
      "2                [0.724644, 0.64715695]         2   \n",
      "3                           [0.1728602]         4   \n",
      "4  [0.17675006, 0.17609319, 0.12565951]         4   \n",
      "\n",
      "                                            gt_boxes  \n",
      "0  [[0.021416015625, 0.02843017578125, 0.02593017...  \n",
      "1  [[0.048435058593750004, 0.053529052734375, 0.0...  \n",
      "2  [[0.1231201171875, 0.024559326171875003, 0.225...  \n",
      "3  [[0.056358642578125, 0.048107910156249996, 0.1...  \n",
      "4  [[0.05265380859375, 0.03525390625, 0.072871093...  \n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions on Test / Validation Set for each image and the metadata\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your best trained model\n",
    "# model = YOLO('yolo_project/train_v12_small/weights/best.pt')\n",
    "\n",
    "# Generator for the results\n",
    "results = model.predict(\n",
    "    # source='data/test/images',              # Path to your test images\n",
    "    source='data/valid/images',              # Path to your test images\n",
    "    stream=True,                            # Stream results to process them one by one\n",
    "    conf=0.01,                              # Cast a wide net for predictions (1% confidence)\n",
    "    # conf=0.25,                                 # Confidence threshold\n",
    "    # save=False,                                # Whether to save the results\n",
    "    # save_txt=False,                            # Whether to save results in txt files\n",
    "    # project='yolo_project/predictions',        # Where to save results\n",
    "    # name='test_results',                       # Name of the results folder\n",
    "    # exist_ok=True                              # Overwrite existing results\n",
    ")\n",
    "\n",
    "data_list = []\n",
    "# Process each result\n",
    "# for result in tqdm(results, desc=\"Processing predictions\"):\n",
    "for result in results:\n",
    "    # Each 'result' corresponds to one image's predictions\n",
    "    image_path = result.path  # Path to the image\n",
    "    image_name = Path(image_path).name  # Extract image name\n",
    "    detections = sv.Detections.from_ultralytics(result)  # Convert to Supervision format\n",
    "\n",
    "    # Before filtering \n",
    "    orig_count = len(detections)  # Number of detections\n",
    "    orig_boxes = detections.xyxy  # Bounding boxes\n",
    "\n",
    "    # Filtering logic\n",
    "\n",
    "    # Filter 1: Confidence threshold\n",
    "    # Only keep detections with confidence > 10% \n",
    "    detections = detections[detections.confidence > 0.1]\n",
    "\n",
    "    # Filter 2: Size threshold (e.g., width and height > 20 pixels)\n",
    "    # Assume a pothole is of certain minimum area size (e.g. 20x20 pixels = 400 pixels)\n",
    "    # detections = detections[ detections.area > 400 ]\n",
    "\n",
    "    # Handle ground truth boxes\n",
    "    label_path = Path(str(image_path).replace('images', 'labels')).with_suffix('.txt')\n",
    "    gt_boxes_xyxy = []\n",
    "    gt_class_ids = []\n",
    "\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            # Parse YOLO format: class_id center_x center_y width height (normalized)\n",
    "            if len(parts) >= 5:\n",
    "                class_id = int(parts[0])\n",
    "                _, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                # Convert Normalized YOLO (xywh) to pixel coordinates (xyxy)\n",
    "                x1 = (x_center - width / 2 ) * width\n",
    "                y1 = (y_center - height / 2) * height\n",
    "                x2 = (x_center + width / 2 ) * width\n",
    "                y2 = (y_center + height / 2) * height\n",
    "                # Here you can store or process the ground truth boxes as needed\n",
    "                gt_boxes_xyxy.append([x1, y1, x2, y2])\n",
    "                gt_class_ids.append(class_id)\n",
    "    # Convert to Numpy for Supervision\n",
    "    if gt_boxes_xyxy:\n",
    "        gt_boxes_np = np.array(gt_boxes_xyxy)\n",
    "        gt_classes_np = np.array(gt_class_ids)\n",
    "        \n",
    "        # Create Supervision Detections object for Ground Truth\n",
    "        gt_detections = sv.Detections(\n",
    "            xyxy=gt_boxes_np,\n",
    "            class_id=gt_classes_np\n",
    "        )\n",
    "    else:\n",
    "        # Handle case with no ground truth labels (empty image)\n",
    "        gt_detections = sv.Detections.empty()\n",
    "\n",
    "    gt_count = len(gt_detections)\n",
    "\n",
    "    # After filtering\n",
    "    filtered_count = len(detections)  # Number of detections after filtering\n",
    "    # Here you can process 'detections' as needed\n",
    "\n",
    "    # Store the data\n",
    "    data_list.append({\n",
    "        'image_name': image_name,\n",
    "        'original_count': orig_count,\n",
    "        'filtered_count': filtered_count,\n",
    "        'original_boxes': orig_boxes,\n",
    "        'filtered_boxes': detections.xyxy,\n",
    "        'confidences': detections.confidence,\n",
    "        # Ground Truth Data\n",
    "        'gt_count': gt_count,\n",
    "        'gt_boxes': gt_detections.xyxy.tolist() if len(gt_detections) > 0 else [],\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(data_list)\n",
    "csv_filename = 'prediction_results_val.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {Path(csv_filename).absolute()}\")\n",
    "\n",
    "print(\"Prediction processing complete!\")\n",
    "print(df.head())\n",
    "# print(df)\n",
    "\n",
    "# print(f\"Image: {image_name}, Detections: {len(detections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548e4ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from yolo_project/train_v12_small/weights/best.pt...\n",
      "Starting processing...\n",
      "Predictions will be saved to: runs/vis_predictions_filtered\n",
      "Ground Truth will be saved to: runs/vis_ground_truth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 248it [01:03,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete!\n",
      "1. Data saved to: analysis_pred_vs_gt.csv\n",
      "2. Prediction images saved to: runs/vis_predictions_filtered (Look for Red boxes)\n",
      "3. Ground Truth images saved to: runs/vis_ground_truth (Look for Green boxes)\n",
      "------------------------------\n",
      "First 5 rows of data:\n",
      "                                           filename  pred_count_filtered  \\\n",
      "0  0106_jpg.rf.94ce9a4d57934a6767648b0de8f32153.jpg                    0   \n",
      "1  0122_jpg.rf.ce18c2ebe76966959cc6d314efa8e2a8.jpg                    3   \n",
      "2  0126_jpg.rf.e2a886338e6d02e063bcc11c8726ad45.jpg                    2   \n",
      "3  0149_jpg.rf.f89e5b8de933145d4d8a6654b5c84db6.jpg                    1   \n",
      "4  0151_jpg.rf.a067a83a8a33184931b64fdfc009eb49.jpg                    3   \n",
      "\n",
      "                                 pred_boxes_filtered  gt_count  \\\n",
      "0                                                 []         2   \n",
      "1  [[134.90692138671875, 549.4906005859375, 258.1...         4   \n",
      "2  [[503.0929870605469, 392.3218994140625, 611.35...         2   \n",
      "3  [[594.559326171875, 624.609375, 639.134765625,...         4   \n",
      "4  [[224.2021942138672, 335.6171875, 338.68121337...         4   \n",
      "\n",
      "                                            gt_boxes  pred_count_raw  \n",
      "0  [[204.00000000000003, 342.5, 247.0, 376.500000...               0  \n",
      "1  [[389.00000000000006, 466.5, 440.0, 513.5], [5...              13  \n",
      "2  [[246.0, 324.5, 451.0, 355.5], [503.0000000000...              14  \n",
      "3  [[168.5, 563.0, 305.5, 598.0], [355.5, 548.5, ...               3  \n",
      "4  [[237.0, 380.0, 328.0, 418.0], [299.5, 346.0, ...               7  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions on Validation Set with Visualization and Metadata Logging\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Assumption: Your data structure is standard YOLO format:\n",
    "# .../data/valid/images/img1.jpg\n",
    "# .../data/valid/labels/img1.txt\n",
    "\n",
    "VALID_IMAGES_DIR = 'data/valid/images'\n",
    "MODEL_PATH = 'yolo_project/train_v12_small/weights/best.pt'\n",
    "\n",
    "# Output directories\n",
    "PRED_OUTPUT_DIR = 'runs/vis_predictions_filtered'\n",
    "GT_OUTPUT_DIR = 'runs/vis_ground_truth'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(PRED_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(GT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- SETUP ---\n",
    "\n",
    "# 1. Load trained model\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "# 2. Define Annotators with distinct colors for visual clarity\n",
    "# Predictions in RED\n",
    "pred_box_annotator = sv.BoxAnnotator(color=sv.Color.RED, thickness=2)\n",
    "pred_label_annotator = sv.LabelAnnotator(color=sv.Color.RED, text_scale=0.5, text_thickness=1)\n",
    "\n",
    "# Ground Truth in GREEN\n",
    "gt_box_annotator = sv.BoxAnnotator(color=sv.Color.GREEN, thickness=2)\n",
    "gt_label_annotator = sv.LabelAnnotator(color=sv.Color.GREEN, text_scale=0.5, text_thickness=1)\n",
    "\n",
    "\n",
    "# 3. Start Inference Generator\n",
    "# We use a very low confidence here (0.01) to get raw data, then filter later\n",
    "results_generator = model.predict(\n",
    "    source=VALID_IMAGES_DIR,\n",
    "    stream=True,\n",
    "    conf=0.01,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "data_list = []\n",
    "\n",
    "print(f\"Starting processing...\")\n",
    "print(f\"Predictions will be saved to: {PRED_OUTPUT_DIR}\")\n",
    "print(f\"Ground Truth will be saved to: {GT_OUTPUT_DIR}\")\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for result in tqdm(results_generator, desc=\"Processing Images\"):\n",
    "    \n",
    "    # A. General Image Info\n",
    "    image_path = Path(result.path)\n",
    "    image_name = image_path.name\n",
    "    \n",
    "    # Load original image matrix (we will create copies later for drawing)\n",
    "    original_frame = result.orig_img\n",
    "    h, w, _ = original_frame.shape\n",
    "\n",
    "    # ============================================\n",
    "    # B. PROCESSING PREDICTIONS\n",
    "    # ============================================\n",
    "    raw_detections = sv.Detections.from_ultralytics(result)\n",
    "    pred_orig_count = len(raw_detections)\n",
    "    \n",
    "    # --- Filtering Logic ---\n",
    "    # 1. Confidence Threshold (> 10%)\n",
    "    filtered_detections = raw_detections[raw_detections.confidence > 0.1]\n",
    "    # 2. Area Threshold (> 400px)\n",
    "    filtered_detections = filtered_detections[filtered_detections.area > 400]\n",
    "    \n",
    "    pred_filtered_count = len(filtered_detections)\n",
    "\n",
    "    # ============================================\n",
    "    # C. PROCESSING GROUND TRUTH\n",
    "    # ============================================\n",
    "    # Construct path to corresponding label text file\n",
    "    label_path = Path(str(image_path).replace('images', 'labels')).with_suffix('.txt')\n",
    "    \n",
    "    gt_boxes_xyxy = []\n",
    "    # We don't necessarily need class IDs for single-class, but good practice to parse\n",
    "    gt_class_ids = [] \n",
    "    \n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Parse YOLO TxT format (normalized cx,cy,w,h) to Pixel format (x1,y1,x2,y2)\n",
    "        for line in lines:\n",
    "            parts = list(map(float, line.strip().split()))\n",
    "            if len(parts) >= 5: # Ensure line isn't malformed\n",
    "                cls_id = int(parts[0])\n",
    "                cx, cy, bw, bh = parts[1], parts[2], parts[3], parts[4]\n",
    "                \n",
    "                x1 = (cx - bw / 2) * w\n",
    "                y1 = (cy - bh / 2) * h\n",
    "                x2 = (cx + bw / 2) * w\n",
    "                y2 = (cy + bh / 2) * h\n",
    "                \n",
    "                gt_boxes_xyxy.append([x1, y1, x2, y2])\n",
    "                gt_class_ids.append(cls_id)\n",
    "\n",
    "    # Create Supervision Detections object for GT\n",
    "    if gt_boxes_xyxy:\n",
    "        gt_detections = sv.Detections(\n",
    "            xyxy=np.array(gt_boxes_xyxy),\n",
    "            class_id=np.array(gt_class_ids)\n",
    "        )\n",
    "    else:\n",
    "        gt_detections = sv.Detections.empty()\n",
    "\n",
    "    gt_count = len(gt_detections)\n",
    "\n",
    "    # ============================================\n",
    "    # D. VISUALIZATION AND SAVING (The new part)\n",
    "    # ============================================\n",
    "    \n",
    "    # 1. VISUALIZE PREDICTIONS (Red)\n",
    "    # Create a fresh copy of the frame so we only draw predictions on it\n",
    "    pred_frame = original_frame.copy()\n",
    "    \n",
    "    # Create labels showing confidence scores\n",
    "    pred_labels = [f\"Pred: {conf:.2f}\" for conf in filtered_detections.confidence]\n",
    "    \n",
    "    # Annotate\n",
    "    pred_frame = pred_box_annotator.annotate(scene=pred_frame, detections=filtered_detections)\n",
    "    pred_frame = pred_label_annotator.annotate(scene=pred_frame, detections=filtered_detections, labels=pred_labels)\n",
    "    \n",
    "    # Save to Prediction folder\n",
    "    cv2.imwrite(os.path.join(PRED_OUTPUT_DIR, image_name), pred_frame)\n",
    "\n",
    "\n",
    "    # 2. VISUALIZE GROUND TRUTH (Green)\n",
    "    # Create another fresh copy of the frame\n",
    "    gt_frame = original_frame.copy()\n",
    "    \n",
    "    # Create simple labels indicating GT\n",
    "    gt_labels_text = [\"GT-Pothole\" for _ in range(len(gt_detections))]\n",
    "    \n",
    "    # Annotate\n",
    "    gt_frame = gt_box_annotator.annotate(scene=gt_frame, detections=gt_detections)\n",
    "    gt_frame = gt_label_annotator.annotate(scene=gt_frame, detections=gt_detections, labels=gt_labels_text)\n",
    "\n",
    "    # Save to Ground Truth folder\n",
    "    cv2.imwrite(os.path.join(GT_OUTPUT_DIR, image_name), gt_frame)\n",
    "\n",
    "\n",
    "    # ============================================\n",
    "    # E. DATA LOGGING\n",
    "    # ============================================\n",
    "    data_list.append({\n",
    "        'filename': image_name,\n",
    "        # Predictions\n",
    "        'pred_count_filtered': pred_filtered_count,\n",
    "        'pred_boxes_filtered': filtered_detections.xyxy.tolist(),\n",
    "        # Ground Truth\n",
    "        'gt_count': gt_count,\n",
    "        'gt_boxes': gt_detections.xyxy.tolist() if gt_count > 0 else [],\n",
    "        # Metadata for checking filter effectiveness\n",
    "        'pred_count_raw': pred_orig_count,\n",
    "    })\n",
    "\n",
    "\n",
    "# === FINISH ===\n",
    "# Create DataFrame and save CSV\n",
    "df = pd.DataFrame(data_list)\n",
    "csv_filename = 'analysis_pred_vs_gt.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(\"\\nProcessing Complete!\")\n",
    "print(f\"1. Data saved to: {csv_filename}\")\n",
    "print(f\"2. Prediction images saved to: {PRED_OUTPUT_DIR} (Look for Red boxes)\")\n",
    "print(f\"3. Ground Truth images saved to: {GT_OUTPUT_DIR} (Look for Green boxes)\")\n",
    "print(\"-\" * 30)\n",
    "print(\"First 5 rows of data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47522c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of potholes detected: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing inference on an image (for demo purposes)\n",
    "# --- REQUIREMENT: \"PERFORM INFERENCE ON AN IMAGE\" ---\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "confidence_threshold = 0.25\n",
    "# # 1. Load your best trained model\n",
    "# model = YOLO('yolo_project/train_v12_small/weights/best.pt')\n",
    "# 2. Load Image\n",
    "\n",
    "# Extract the original filename without extension\n",
    "image_location = 'demo/demo001.jpg'\n",
    "input_filename = Path(image_location).stem  # Gets 'demo001'\n",
    "\n",
    "image = cv2.imread(image_location)\n",
    "\n",
    "\n",
    "# 3. Run YOLO Prediction\n",
    "results = model(image, conf = confidence_threshold)[0]\n",
    "# 4. Convert to Supervision format (The Magic Step)\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "# --- IGNORE ---\n",
    "pothole_count = len(detections)\n",
    "print(f\"Number of potholes detected: {pothole_count}\")\n",
    "# --- IGNORE ---\n",
    "# 5. Filter (Optional): Remove low confidence detections\n",
    "# detections = detections[detections.confidence > confidence_threshold]\n",
    "# 6. Draw Boxes and Labels\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "# Draw the boxes on the image\n",
    "annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "# Draw the labels (e.g., \"Pothole 0.85\")\n",
    "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "# 7. Show or Save\n",
    "# Save with the original name in the output\n",
    "output_path = f'{input_filename}_annotated.jpg'\n",
    "# cv2.imwrite(f'output_with_supervision.jpg', annotated_image)\n",
    "cv2.imwrite(output_path, annotated_image)\n",
    "# print(f\"Saved output image: {output_path}\")\n",
    "# print(\"Saved output image!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
